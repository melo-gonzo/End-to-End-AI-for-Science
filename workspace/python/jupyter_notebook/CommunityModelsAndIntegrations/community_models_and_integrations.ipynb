{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8005778a",
   "metadata": {},
   "source": [
    "# A Tutorial on Community Models and Integrations with PhysicsNeMo\n",
    "Scientific machine learning (SciML) is becoming a fundamental part of of research, development, and discovery workflows for scientists across many domains such as computational fluid dynamics, materials characterization, climate and weather modeling, and computer aided engineering. With the increase use of AI and ML in these domains also comes an increase in the number of resources available for developing models, and a plethora of datasets to use as starting points for model training. While many efforts are segmented and tailor-made for specific use cases, projects such as PhysicsNeMo, The Well, and Proxima Fusion's ConStellaration dataset are great examples of community efforts to bridge the gap between siloed SciML research and collaborative community projects.\n",
    "\n",
    "In this tutorial, PhysicsNeMo is used to expand the scope of community models and datasets from [The Well](https://github.com/PolymathicAI/the_well) and the [ConStellaration Challenge](https://huggingface.co/blog/cgeorgiaw/constellaration-fusion-challenge) by leveraging pre-trained physics informed machine learning models, community accessible datasets, and the robust SciML framework from PhysicsNeMo.\n",
    "\n",
    "\n",
    "Specifically, this tutorial covers:\n",
    "\n",
    "* [Loading data from The Well](#loading-models-and-data-from-the-well)\n",
    "* [Training a PhysicsNeMo model using data from The Well](#using-the-well-data-to-train-physicsnemo-models)\n",
    "* [Running models from The Well in the PhysicsNeMo framework](#running-models-from-the-well-in-physicsnemo)\n",
    "* [Fine-tuning models from The Well using PhysicsNeMo](#fine-tuning-models-from-the-well-in-physicsnemo)\n",
    "* [Exploring a community design challenge for fusion research and engineering](#exploring-the-huggingface-stellerator-design-dataset)\n",
    "\n",
    "Note that the recommended hardware for this tutorial is at least 100GB of free disk space, and a GPU with at least 24GB VRAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa944336",
   "metadata": {},
   "source": [
    "## Loading Models and Data from The Well\n",
    "\n",
    "The Well is large-scale collection of machine learning datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. Additionally, a variety of state-of-the-art models are included, with a large selection of pre-trained models available for download on HuggingFace. Specifically, The Well includes 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, and magnetohydrodynamics simulations. \n",
    "\n",
    "In this tutorial, the focus will largely be on magnetohydrodynamics. Magnetohydrodynamics (MHD), is the study of the dynamics of electrically conducting fluids such as plasmas. Its applications range from understanding the flow of plasmas in the Sun, to simulating the physics inside of magnetically confined fusion devices. The dynamics of these systems are represented by combining the Naiver-Stokes equations with Maxwells equations - capturing both fluid flows and electromagnetic forces. \n",
    "\n",
    "\n",
    "While the specific domain focus of this tutorial is only on MDH and its applications, the workflows and integrations described are transferable to other domains, datasets, and models. \n",
    "\n",
    "The tutorial will specifically make use of the [MHD_64 dataset](https://github.com/PolymathicAI/the_well/tree/master/datasets/MHD_64).  \n",
    "\n",
    "Datasets from The Well are available through HuggingFace, and can either be streamed via the `datasets` API, or saved locally. The command to download all splits is `the-well-download --base-path path/to/base --dataset MHD_64`. This dataset is around 71GB and may take a up to an hour to download and save locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affec459",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "To quickly examine the dataset, a single sample can be loaded from the streaming API. The [data-card](https://huggingface.co/datasets/polymathic-ai/MHD_64) available on HuggingFace provides additional information about the simulations, equations modeled, and a reference paper for in depth explanation of the physics of the problem. A snipped for loading the dataset and details from the data-card are provided for reference.\n",
    "\n",
    "Note that in the following code, either streaming or loading from a locally saved version of the dataset is supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3381f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from the_well.data import WellDataset\n",
    "from the_well.data.normalization import ZScoreNormalization\n",
    "from torchinfo import summary\n",
    "\n",
    "use_streaming = True\n",
    "\n",
    "# Enable streaming the dataset from HuggingFace or loading from local directory\n",
    "# The following line may take a couple of minutes to instantiate the datamodule\n",
    "dataset = WellDataset(\n",
    "    well_base_path=\"hf://datasets/polymathic-ai/\" if use_streaming else \"./TheWellMHDData/datasets\",\n",
    "    well_dataset_name=\"MHD_64\",\n",
    "    well_split_name=\"train\",\n",
    "    use_normalization=True,\n",
    "    normalization_type=ZScoreNormalization,\n",
    "    n_steps_input=4,\n",
    "    n_steps_output=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7787d",
   "metadata": {},
   "source": [
    "With the dataset on hand, its features, shape and size can be explored. The Well provides a great script for this already, and is left to the reader to go through if desired. A summary is provided below, also available on the [data-card](https://github.com/PolymathicAI/the_well/blob/master/datasets/MHD_64/README.md) online:\n",
    "\n",
    "* **Dimension of discretized data:** 100 time steps of 64 $\\times$ 64 $\\times$ 64 cubes.\n",
    "* **Fields available in the data:** Density (scalar field), velocity (vector field), magnetic field (vector field).\n",
    "* **Number of trajectories:** 10 Initial conditions x 10 combination of parameters = 100 trajectories.\n",
    "* **Estimated size of the ensemble of all simulations:** 71.6 GB.\n",
    "* **Grid type:** uniform grid, cartesian coordinates.\n",
    "* **Initial conditions:** uniform IC.\n",
    "* **Boundary conditions:** periodic boundary conditions.\n",
    "* **Data are stored separated by ($\\Delta t$):** 0.01 (arbitrary units).\n",
    "* **Total time range ($t\\_{min}$ to $t\\_{max}$):** $t\\_{min} = 0$, $t\\_{max} = 1$.\n",
    "* **Spatial domain size ($L_x$, $L_y$, $L_z$):** dimensionless so 64 pixels.\n",
    "* **Set of coefficients or non-dimensional parameters evaluated:** all combinations of $\\mathcal{M}_s=${0.5, 0.7, 1.5, 2.0 7.0} and $\\mathcal{M}_A =${0.7, 2.0}.\n",
    "* **Approximate time and hardware used to generate the data:** Downsampled from `MHD_256` after applying ideal low-pass filter.\n",
    "* **What phenomena of physical interest are catpured in the data:** MHD fluid flows in the compressible limit (sub and super sonic, sub and super Alfvenic).\n",
    "* **How to evaluate a new simulator operating in this space:** Check metrics such as Power spectrum, two-points correlation function.\n",
    "\n",
    "Please cite the associated paper if you use this data in your research:\n",
    "\n",
    "```\n",
    "@article{burkhart2020catalogue,\n",
    "  title={The catalogue for astrophysical turbulence simulations (cats)},\n",
    "  author={Burkhart, B and Appel, SM and Bialy, S and Cho, J and Christensen, AJ and Collins, D and Federrath, Christoph and Fielding, DB and Finkbeiner, D and Hill, AS and others},\n",
    "  journal={The Astrophysical Journal},\n",
    "  volume={905},\n",
    "  number={1},\n",
    "  pages={14},\n",
    "  year={2020},\n",
    "  publisher={IOP Publishing}\n",
    "}\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44dbf1",
   "metadata": {},
   "source": [
    "A single sample can be extracted and inspected. Some notes from The Well are shown below. More info on examining their data can be found in [this example](https://github.com/PolymathicAI/the_well/blob/master/docs/tutorials/dataset.ipynb):\n",
    "\n",
    "The most important elements are `input_fields` and `output_fields`. They represent the time-varying physical fields of the dynamical system and are generally the input and target output of our models. For a dynamical system that has 3 spatial dimensions $x$, $y$, and $z$, `input_fields` would have a shape $(T_{in}, L_x, L_y, L_z, F)$ and `output_fields` would have a shape $(T_{out}, L_x, L_y, L_z, F)$. The number of input and output time steps $T_{in}$ and $T_{out}$ are specified at the instantiation of the dataset with the arguments `n_steps_input` and `n_steps_output`. $L_x$, $L_y$ and $L_z$ are the lengths of the spatial dimensions. $F$ represents the number of physical fields, where vector fields $v = (v_x, v_y, v_z)$ and tensor fields $t = (t_{xx}, t_{xy}, t_{xz}, t_{yy}, t_{yx}, t_{yz},  t_{zz}, t_{zx}, t_{zy})$ are flattened.\n",
    "\n",
    "Note that the MHD_64 dataset only contains scalar and vector fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef513f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total number of samples: {len(dataset)}\")\n",
    "sample = dataset[0]\n",
    "for k, v in sample.items():\n",
    "    print(f\"Key: {k.ljust(20)} Shape: {v.shape}\")\n",
    "\n",
    "print(f\"Field Names: {dataset.metadata.field_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915ac36-ea41-4821-b38e-1e874cb8da12",
   "metadata": {},
   "source": [
    "## Using The Well Data to Train PhysicsNeMo Models\n",
    "\n",
    "With the dataset prepared, a model can be trained to approximate the system dynamics, resulting in a surrogate model for magnetohydrodynamics simulations. To align with models from The Well, the PhysicsNeMo Tensor-Factorized Fourier Neural Operator implementation is adapted to closely match the implementation from The Well. The model utilizes Tucker factorization, and has around 300M parameters. \n",
    "\n",
    "For the remainder of this tutorial, a boiler-plate `Trainer` class is implemented in `training_utils.py` that contains the core components needed for loading models, running training loops, saving checkpoints, evaluating models, etc. The method `setup_model` needs to be implemented in order to attach a model to the Trainer. For completeness, the TFNO architecture from `PhysicsNeMo` is included in the `tfno` folder, which is initialized with parameters that closely match with the defaults from The Well. The config file used to define the parameters for our model, data, and trainer are in the `config` folder.\n",
    "\n",
    "The PhysicsNeMo framework utilizes the Hydra configuration framework, enabling streamlined tracking of all parameters associated with running experiments, and is a common best-practice in machine learning workflows. Additionally, the `training_utils.py` makes use of other best-practices such as metric logging, checkpointing, and distributed workflow utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd6ad7-efaf-4113-82f8-0f79d4e46b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from training_utils import Trainer\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "\n",
    "initialize(version_base=None, config_path=\"./config\")\n",
    "cfg = compose(config_name=\"mhd_config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41775e6",
   "metadata": {},
   "source": [
    "\n",
    "Note that the `training_utils.py` script can be used either by calling it directly or through `torchrun`, however the `setup_model` method will need to be implemented if using this approach.\n",
    "```python \n",
    "python training_utils.py\n",
    "```\n",
    "or \n",
    "```python\n",
    "torchrun --standalone --nnodes=1 --nproc_per_node=1 training_utils.py\n",
    "```\n",
    "\n",
    "In the following code, the TFNO model from PhysicsNeMo is set up in a child class of the `Trainer`, which instantiates the model through various model parameters set in the Hydra config file. A model summary is printed for convenience and reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09dee18-82aa-474f-8cb8-6d0de3cc5203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHDTrainer(Trainer):\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the TFNO model based in PhysicsNeMo. Input parameters are set and controlled by the yaml config.\"\"\"\n",
    "        from tfno.tfno import TFNO\n",
    "\n",
    "        self.model = TFNO(\n",
    "            in_channels=self.model_params.in_dim,\n",
    "            coord_features=self.model_params.coord_features,\n",
    "            out_channels=self.model_params.out_dim,\n",
    "            decoder_layers=self.model_params.decoder_layers,\n",
    "            decoder_layer_size=self.model_params.fc_dim,\n",
    "            dimension=self.model_params.dimension,\n",
    "            latent_channels=self.model_params.layers,\n",
    "            num_fno_layers=self.model_params.num_fno_layers,\n",
    "            num_fno_modes=self.model_params.modes,\n",
    "            padding=[\n",
    "                self.model_params.pad_z,\n",
    "                self.model_params.pad_y,\n",
    "                self.model_params.pad_x,\n",
    "            ],\n",
    "            rank=self.model_params.rank,\n",
    "            factorization=self.model_params.factorization,\n",
    "            fixed_rank_modes=self.model_params.fixed_rank_modes,\n",
    "            decomposition_kwargs=self.model_params.decomposition_kwargs,\n",
    "        ).to(self.dist.device)\n",
    "\n",
    "        print(summary(self.model, depth=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab91f1e",
   "metadata": {},
   "source": [
    "Finally, the model can be trained. Note that one of the config parameters is overwritten here for convenience, which could have optionally be set directly in the `.yaml` file itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d158042-3c58-4079-8196-206ba038f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train_params.ckpt_path = \"./checkpoints/pnm_model_well_data\"\n",
    "mhd_trainer = MHDTrainer(cfg)\n",
    "mhd_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d75b9",
   "metadata": {},
   "source": [
    "During training, checkpoints are saved at the specified `ckpt_freq`, which can be loaded with the same `MDHTrainer` to use for fine-tuning, inference or evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc37d72-df24-4944-8d3a-37dcc0f65758",
   "metadata": {},
   "source": [
    "## Running Models From The Well in PhysicsNemo\n",
    "\n",
    "PhysicsNeMo offers great utility in running community models natively by way of a simple conversion. The documentation for this can be found [here](https://docs.nvidia.com/deeplearning/physicsnemo/physicsnemo-core/api/physicsnemo.models.html#converting-pytorch-models-to-physicsnemo-models), with the general steps outlined below. Before working through the conversion process, the model interface to The Well is explored to understand how these models are used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3a9ec3",
   "metadata": {},
   "source": [
    "Models from The Well can be loaded through their benchmark API. The vanilla PyTorch model is used with the `Module.from_torch` method to pull the model into the PhysicsNeMo framework. The `ModelMetaData` class allows for various optimization strategies to be included in this conversion, including just-in-time compilation, automatic mixed precision training, and cuda-graphs.\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class GenericModelMetadata(ModelMetaData):\n",
    "    name: str = \"GenericModel\"\n",
    "    # Optimization\n",
    "    jit: bool = True\n",
    "    cuda_graphs: bool = True\n",
    "    amp_cpu: bool = True\n",
    "    amp_gpu: bool = True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af4d77d-c0d8-402b-bc61-b79f92e9e6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from physicsnemo.models.meta import ModelMetaData\n",
    "from physicsnemo.models.module import Module\n",
    "from the_well.benchmark.models import TFNO\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WellMetaData(ModelMetaData):\n",
    "    name: str = \"WellTFNOModel\"\n",
    "\n",
    "\n",
    "well_nemo_model = Module.from_torch(TFNO, meta=WellMetaData())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f1c1b9",
   "metadata": {},
   "source": [
    "At this stage, the `well_nemo_model` still needs to be initialized with model parameters. We can simply use:\n",
    "```python\n",
    "instantiated_model = well_nemo_model(**parameters)\n",
    "```\n",
    "\n",
    "Alternatively, the PhysicsNeMo model registry can be used to load in the newly created model. In the following cell, the model is instantiated through the model registry, however either method is valid. The model registry in PhysicsNeMo provides access to a variety of architectures that can be used for SciML tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1fdfb9-f4ae-4d46-874c-5f7b703967f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physicsnemo.registry import ModelRegistry\n",
    "\n",
    "ModelRegistry().list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54301d85-7faf-4ae0-8345-4991652edd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from physicsnemo.registry import ModelRegistry\n",
    "\n",
    "well_nemo_model = ModelRegistry().factory(\"TFNOPhysicsNeMoModel\")(\n",
    "    dim_in=28,\n",
    "    dim_out=7,\n",
    "    n_spatial_dims=3,\n",
    "    spatial_resolution=[64, 64, 64],\n",
    "    hidden_channels=128,\n",
    "    modes1=16,\n",
    "    modes2=16,\n",
    "    modes3=16,\n",
    ")\n",
    "\n",
    "summary(well_nemo_model, depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8351d103",
   "metadata": {},
   "source": [
    "By examining the model summary from above and the order of operations in the [TFNO forward pass](https://github.com/neuraloperator/neuraloperator/blob/7bb578df787d6ca548b623b83f0601c98dc931fb/neuralop/models/fno.py#L337), the data is processed by:\n",
    "\n",
    "1. Applying optional positional encoding\n",
    "2. Sending inputs through a lifting layer to a high-dimensional latent space\n",
    "3. Applying optional domain padding to high-dimensional intermediate function representation\n",
    "4. Applying `n_layers` Fourier/TFNO layers in sequence (SpectralConvolution + skip connections, nonlinearity) \n",
    "5. If domain padding was applied, domain padding is removed\n",
    "6. Projection of intermediate function representation to the output channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77219d3f",
   "metadata": {},
   "source": [
    "The Well benchmark aims to showcase the effectiveness of applying SoTA models to the forward problem: predicting the next step of a simulation from a history of 4 previous time steps. Trained models can then be used in an autoregressive setting by predicting next timesteps, and the concatenating these predictions back into the input.\n",
    "\n",
    "Concretely, the MHD TFNO model is used to predict the $T_{out} = 1$ next states given the $T_{in} = 4$ previous states. The input steps are concatenated along their channels, such that the model expects $T_{in} \\times F$ channels as input and $T_{out} \\times F$ channels as output. This introduces an assumption of our models that four timesteps will be available to use as input and can limit the application of these models to real world use cases, however this serves as an initial starting point towards building surrogate models for this dynamical system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4248391-ef8f-413e-a29b-5c307cb40e2b",
   "metadata": {},
   "source": [
    "To now train this model in PhysicsNeMo with the same dataset from The Well, the `Trainer` class can be updated to perform the same model loading and conversion that is required to pull the model into PhysicsNeMo framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661e334-66e4-4f6c-ae6c-6ddf8fb0fd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHDTrainer(Trainer):\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the TFNO model based in PhysicsNeMo. Input parameters are set and controlled by the yaml config.\"\"\"\n",
    "        from dataclasses import dataclass\n",
    "\n",
    "        from physicsnemo.models.meta import ModelMetaData\n",
    "        from physicsnemo.models.module import Module\n",
    "        from the_well.benchmark.models import TFNO\n",
    "\n",
    "        @dataclass\n",
    "        class WellMetaData(ModelMetaData):\n",
    "            name: str = \"WellTFNOModel\"\n",
    "\n",
    "        well_nemo_model = Module.from_torch(TFNO, meta=WellMetaData())\n",
    "        well_nemo_model = well_nemo_model(\n",
    "            dim_in=28,\n",
    "            dim_out=7,\n",
    "            n_spatial_dims=3,\n",
    "            spatial_resolution=[64, 64, 64],\n",
    "            hidden_channels=128,\n",
    "            modes1=16,\n",
    "            modes2=16,\n",
    "            modes3=16,\n",
    "        )\n",
    "\n",
    "        self.model = well_nemo_model.to(self.dist.device)\n",
    "        print(summary(self.model, depth=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc308aaf",
   "metadata": {},
   "source": [
    "Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e0a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train_params.ckpt_path = \"./checkpoints/well_model_well_data\"\n",
    "well_model_trainer = MHDTrainer(cfg)\n",
    "well_model_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43eda699",
   "metadata": {},
   "source": [
    "Similar to the previous section, checkpoints are available for the trained model in the specified directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72959594-a700-4496-a66d-efbcf06b4a8a",
   "metadata": {},
   "source": [
    "## Fine-tuning Models from The Well in PhysicsNeMo\n",
    "One great feature of The Well is the abundance of pre-trained models that they provided, typically offering [many pretrained model architectures](https://huggingface.co/collections/polymathic-ai/the-well-benchmark-models-67e69bd7cd8e60229b5cd43e) for a single dataset. These pre-trained models can be fine-tuned using PhysicsNeMo by leveraging a similar approach to the above example - converting a model into the PhysicsNeMo format. \n",
    "\n",
    "In this section, the pre-trained TFNO model will be loaded from The Well, converted to a PhysicsNeMo model, and then used in the context of transfer learning to adapt the model to a new task with yet another community dataset.\n",
    "\n",
    "The new dataset will come from a HuggingFace and Proxima Fusion collaboration project: [a community challenge focused around Stellarator design](https://huggingface.co/blog/cgeorgiaw/constellaration-fusion-challenge). In the field of magnetically confined fusion, the physics of magnetohydrodynamics influence the system in many ways, and modeling these equations help researchers and engineers understand reaction ability, equipment design, and plasma shapes, to name a few. In the remainder of this tutorial, we will set up the trainer, inspect the Stellarator design dataset, and use transfer learning to fine-tune a model for a new task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4de9c4",
   "metadata": {},
   "source": [
    "### Loading The Well Pre-Trained Checkpoint as PhysicsNeMo Model\n",
    "\n",
    "Models from The Well benchmark are available through HuggingFace, and can be trivially loaded with just a few lines of code:\n",
    "\n",
    "```python\n",
    "from the_well.benchmark.models import TFNO\n",
    "\n",
    "well_model = TFNO.from_pretrained(\"polymathic-ai/TFNO-MHD_64\")\n",
    "```\n",
    "\n",
    "In order to use this model with PhysicsNeMo, the base TFNO pytorch model needs to first be converted to PhysicsNeMo format in the same way as the previous section. After this, the pre-trained model parameters can be directly transferred to the PhysicsNeMo model. To ensure the model is instantiated correctly the following steps can be used:\n",
    "* Collect all of the model hyperparameters from the pre-trained model instance\n",
    "* Inspect the TFNO class for its required input arguments\n",
    "* Instantiate the PhysicsNeMo model with the required TFNO arguments and their values from the pre-trained model.\n",
    "\n",
    "The explicit steps for this procedure are outlined in the `setup_model` method of `MHDTrainer` below. \n",
    "\n",
    "Note that this procedure is general and can be applied other models from within The Well, as well as additional community models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa4ea3-c5f8-4741-bac4-9a847b150a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHDTrainer(Trainer):\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the pretrained TFNO model from The Well as a PhysicsNeMo model.\n",
    "        Model input parameters are set from the well, and the original config is updated to reflect this.\"\"\"\n",
    "        import inspect\n",
    "\n",
    "        from physicsnemo.models.meta import ModelMetaData\n",
    "        from physicsnemo.models.module import Module\n",
    "        from the_well.benchmark.models import TFNO\n",
    "\n",
    "        well_model = TFNO.from_pretrained(\"polymathic-ai/TFNO-MHD_64\")\n",
    "        model_dict = well_model.__dict__\n",
    "\n",
    "        signature = inspect.signature(TFNO)\n",
    "        parameters = signature.parameters\n",
    "        filtered_params = {k: model_dict[k] for k in parameters if k in model_dict}\n",
    "\n",
    "        model = Module.from_torch(TFNO, meta=ModelMetaData(name=\"converted_tfno\"))\n",
    "        well_pretrained_model = model(**filtered_params)\n",
    "        well_pretrained_model.inner_model.load_state_dict(\n",
    "            well_model.state_dict(), strict=True\n",
    "        )\n",
    "        self.model = well_pretrained_model.to(self.dist.device)\n",
    "\n",
    "        print(summary(self.model, depth=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a25d60",
   "metadata": {},
   "source": [
    "With a community pre-trained model now available trough the PhysicsNeMo framework! It can now be applied to downstream tasks such as fine-tuning with a custom dataset, or using the pre-trained model as as surrogate in simulation. Note that if starting down the path of fine-tuning, custom dataloaders may needed if working with dataset outside of The Well and PhysicsNeMo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c05319-1bdd-400e-8b84-f96dd4faba04",
   "metadata": {},
   "source": [
    "### Exploring the HuggingFace Stellerator Design Dataset\n",
    "With resources for a simple framework for using community datasets and models with PhysicsNeMo, some deeper problems can be explored. In this section, the pre-trained TFNO model for magnetohydrodynamics will be augmented, and used as a foundation for transfer learning onto a new dataset with a related objective: optimizing the design of a stellarator. Stellarators are a type of magnetically confined fusion device for containing plasma of very high temperature and pressure. The design of these devices is complex due to their geometry. A recent challenge on HuggingFace serves as the basis for this transfer learning problem.\n",
    "\n",
    "From the challenge page:\n",
    "\n",
    "*\"The ConStellaration dataset contains over 150,000 QI equilibria produced by VMEC++. As a reminder, QI stellarators are a subset of configurations that minimize the internal plasma currents that can lead to disruptive events in a tokamak. The provided equilibria correspond to different 3D plasma boundary surfaces and offer samples across a wide and physically meaningful range of parameters. The dataset includes:*\n",
    "\n",
    "*Input parameters: the 3D plasma boundary, together with the pressure and current profiles.*\n",
    "*Equilibrium outputs: full VMEC++ equilibrium solution plus additional metrics of interest for stellarator design (e.g., degree of QI symmetry, turbulent transport geometrical quantities).\"*\n",
    "\n",
    "\n",
    "There are many ways in which to use the dataset, and namely there are three unique challenges present that are available to solve, starting with low complexity and moving up to multimodal optimization problems. The three challenge problems involve optimizing the stellarator geometry to minimize or maximize certain related parameters such as minimizing elongation under fixed constraints, optimizing plasma shapes for confinement, and balancing compactness and simplicity.\n",
    "\n",
    "\n",
    "These challenges serve a great problem to use as an end goal for combing these multiple frameworks, and while this example will not directly solve any of the stated challenges, it will serve as a starting point towards utilizing community models and datasets with PhysicsNeMo to solve real world problems.\n",
    "\n",
    "The goal of the model in this case is to serve as a surrogate for approximating the equilibrium dynamics of the plasma, which then maps to quantities of interest. The dataset contains input geometry and a large list of output parameters that can be learned. Because the pre-trained TFNO model already has learned knowledge of magnetohydrodynamics, it is used to translate the input design parameters into predicted properties, by way of mapping input geometry to predicted MHD equilibrium field, and then onto the properties of interest. The model here will utilize the pre-trained TFNO as a frozen intermediate surrogate model, while the input and output projections will be learned during training. \n",
    "\n",
    "Specifically, the input geometry of the stellarator plasma is converted from Fourier coefficients defining its boundaries into a 3D representation, and the output is the predicted max elongation of the plasma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173ec6e6",
   "metadata": {},
   "source": [
    "The dataloader class is provided in `constellaration_dataloader.py` provides a base implementation that can be used to start working on machine learning models for predicting properties of stellarators. The `FullConstellarationDataLoader` transforms the Fourier coefficients in to a 3D representation of the stellarator boundary, and the `ConstellarationDataLoader` simply returns a concatenated list of the coefficients. In this example, the mapping of Fourier coefficients to 3D space is utilized to ensure closer alignment with the intermediate surrogate model. An example from the dataset is loaded and visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce38ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from constellaration_dataloader import ConstellarationDataLoader\n",
    "\n",
    "# Initialize the dataloader for challenge_1 with full dataset type\n",
    "dataloader = ConstellarationDataLoader(\n",
    "    challenge=\"challenge_1\",\n",
    "    dataset_type=\"full\",\n",
    "    grid_size=(64, 64, 64),\n",
    "    batch_size=1,\n",
    ")\n",
    "\n",
    "# Prepare the dataset\n",
    "dataloader.prepare()\n",
    "\n",
    "# Get a single sample from the training dataset\n",
    "sample = dataloader.train_dataset[0]\n",
    "\n",
    "# Extract the 3D volume data and metric value\n",
    "input_fields = sample[\"input_fields\"]  # Shape: [3, Nt, Np, Nrho]\n",
    "output_fields = sample[\"output_fields\"]  # Shape: [1] for challenge_1\n",
    "\n",
    "# Get the metric value (max_elongation for challenge_1)\n",
    "max_elongation = output_fields.item()\n",
    "\n",
    "# Extract the surface points (rho=1, which is the last index)\n",
    "# The input_fields contains [X, Y, Z] coordinates\n",
    "X_surface = input_fields[0, :, :, -1].cpu().numpy()  # X coordinates at surface\n",
    "Y_surface = input_fields[1, :, :, -1].cpu().numpy()  # Y coordinates at surface\n",
    "Z_surface = input_fields[2, :, :, -1].cpu().numpy()  # Z coordinates at surface\n",
    "\n",
    "# Create the 3D plot\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "# Plot the surface\n",
    "surface = ax.plot_surface(\n",
    "    X_surface,\n",
    "    Y_surface,\n",
    "    Z_surface,\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.8,\n",
    "    linewidth=0,\n",
    "    antialiased=True,\n",
    ")\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(surface, ax=ax, shrink=0.5, aspect=5)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "ax.set_title(f\"3D Stellarator Surface - Max Elongation: {max_elongation:.4f}\")\n",
    "\n",
    "ax.view_init(elev=20, azim=45)\n",
    "ax.set_box_aspect([1, 1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig(\"3D_Stellarator_Surface.png\")\n",
    "\n",
    "# Print some information about the sample\n",
    "print(f\"Sample metric (max_elongation): {max_elongation:.4f}\")\n",
    "print(f\"Surface shape: {X_surface.shape}\")\n",
    "print(f\"X range: [{X_surface.min():.3f}, {X_surface.max():.3f}]\")\n",
    "print(f\"Y range: [{Y_surface.min():.3f}, {Y_surface.max():.3f}]\")\n",
    "print(f\"Z range: [{Z_surface.min():.3f}, {Z_surface.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd27fa",
   "metadata": {},
   "source": [
    "This visualization shows the surface of the stellarator plasma boundary that has an associated `max_elongation` property associated with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cef8ed",
   "metadata": {},
   "source": [
    "The model used to predict `max_elongation` from the input plasm boundary surface is provided below. The pre-trained model from The Well is utilized as an intermediate surrogate model for approximating the equilibrium fields. The models operation is structured into three distinct stages: input projection, pre-trained surrogate core, and output projection. The model first processes the input through an adapter. The `input_projection` module is a small 3D CNN that transforms the initial 3-channel input tensor into the 28-channel format expected by the core surrogate model. The intermediate surrogate model is the same TFNO from The Well that was previously explored. The weights of this core model are frozen, meaning it is not trained further. Instead, it functions as a fixed feature extractor for processing the adapted input. Finally, the model converts the high-dimensional 3D tensor from the TFNO into a simple vector output. This is done by reducing the surrogates 7-channel output into a single channel feature map, using a global pooling of features, flattening into a vector, and processing through a small MLP that maps to the desired output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c778bfc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TFNOSurrogate(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_channels=3,\n",
    "        output_dim=3,\n",
    "        hidden_dim=256,\n",
    "        target_input_channels=28,\n",
    "        target_output_channels=7,\n",
    "        image_size=64,  # Assuming square HxW dimensions\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.target_input_channels = target_input_channels\n",
    "        self.target_output_channels = target_output_channels\n",
    "\n",
    "        # Input projection: 3D conv to project from input_channels to target_input_channels\n",
    "        self.input_projection = nn.Sequential(\n",
    "            nn.Conv3d(input_channels, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv3d(hidden_dim, hidden_dim // 2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv3d(hidden_dim // 2, target_input_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "        # Output projection: Reduce channels to 1 and pool along the depth axis\n",
    "        self.output_projection = nn.Sequential(\n",
    "            # Reduce channels from 7 to 1. Shape: [B, 7, 64, 64, 64] -> [B, 1, 64, 64, 64]\n",
    "            nn.Conv3d(target_output_channels, 1, kernel_size=1),\n",
    "            # Pool only the depth dim. Shape: [B, 1, 64, 64, 64] -> [B, 1, 64, 64, 1]\n",
    "            nn.AdaptiveAvgPool3d((None, None, 1)),\n",
    "        )\n",
    "\n",
    "        # Calculate the size of the flattened 2D feature map\n",
    "        flat_feature_dim = image_size * image_size\n",
    "\n",
    "        # Final projection to output dimension\n",
    "        self.final_projection = nn.Sequential(\n",
    "            nn.Linear(flat_feature_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, output_dim),\n",
    "        )\n",
    "\n",
    "        self.surrogate = self.setup_model()  # pretrained TFNOPhysicsNeMoModel\n",
    "        self.freeze_surrogate_weights()\n",
    "\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the pretrained TFNO model from The Well as a PhysicsNeMo model.\"\"\"\n",
    "        import inspect\n",
    "\n",
    "        from physicsnemo.models.meta import ModelMetaData\n",
    "        from physicsnemo.models.module import Module\n",
    "        from the_well.benchmark.models import TFNO\n",
    "\n",
    "        well_model = TFNO.from_pretrained(\"polymathic-ai/TFNO-MHD_64\")\n",
    "        model_dict = well_model.__dict__\n",
    "\n",
    "        signature = inspect.signature(TFNO)\n",
    "        parameters = signature.parameters\n",
    "        filtered_params = {k: model_dict[k] for k in parameters if k in model_dict}\n",
    "\n",
    "        model = Module.from_torch(TFNO, meta=ModelMetaData(name=\"converted_tfno\"))\n",
    "        well_pretrained_model = model(**filtered_params)\n",
    "        well_pretrained_model.inner_model.load_state_dict(\n",
    "            well_model.state_dict(), strict=True\n",
    "        )\n",
    "        return well_pretrained_model\n",
    "\n",
    "    def freeze_surrogate_weights(self):\n",
    "        \"\"\"Freeze all parameters in the pretrained TFNO surrogate model.\"\"\"\n",
    "        for param in self.surrogate.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.surrogate.eval()\n",
    "        print(\n",
    "            f\"Frozen {sum(p.numel() for p in self.surrogate.parameters())} parameters in pretrained TFNO model\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input projection: [B, 3, 64, 64, 64] -> [B, 28, 64, 64, 64]\n",
    "        x_projected = self.input_projection(x)\n",
    "\n",
    "        # Pass through the pretrained TFNO model: [B, 28, 64, 64, 64] -> [B, 7, 64, 64, 64]\n",
    "        tfno_output = self.surrogate(x_projected)\n",
    "\n",
    "        # Project output to a 2D feature map: [B, 7, 64, 64, 64] -> [B, 1, 64, 64, 1]\n",
    "        projected_2d = self.output_projection(tfno_output)\n",
    "\n",
    "        # Flatten the 2D feature map: [B, 1, 64, 64, 1] -> [B, 4096]\n",
    "        flattened = torch.flatten(projected_2d, start_dim=1)\n",
    "\n",
    "        # Final projection to output dimension: [B, 4096] -> [B, 3]\n",
    "        output = self.final_projection(flattened)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36431ba1",
   "metadata": {},
   "source": [
    "The model and dataloader can finally be used with the previous training utilities and a few slight modifications to train a model for this task. Namely, the `setup_model`, `_setup_data`, and `_forward` methods are updated to accommodate the new model and dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7be03",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class ConstellarationTrainer(Trainer):\n",
    "    def setup_model(self):\n",
    "        \"\"\"Setup the TFNO model based in PhysicsNeMo. Input parameters are set and controlled by the yaml config.\"\"\"\n",
    "        from constellaration_surrogate import TFNOSurrogate\n",
    "\n",
    "        self.model = TFNOSurrogate(input_channels=3, output_dim=1)\n",
    "        self.model.to(self.dist.device)\n",
    "\n",
    "    def _setup_data(self):\n",
    "        from constellaration_dataloader import ConstellarationDataLoader\n",
    "\n",
    "        self.datamodule = ConstellarationDataLoader(\n",
    "            challenge=\"challenge_1\",\n",
    "            dataset_type=\"full\",\n",
    "            grid_size=(64, 64, 64),\n",
    "            batch_size=2,\n",
    "        )\n",
    "        self.datamodule.prepare()\n",
    "\n",
    "    def _forward_pass(self, batch: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # Add dimension for our one input field\n",
    "        inputs = batch[\"input_fields\"].unsqueeze(-1)\n",
    "        # Rearrange for model: (batch, time, x, y, z, fields) -> (batch, (time fields), x, y, z)\n",
    "        model_inputs = rearrange(inputs, \"B Ti Lx Ly Lz F -> B (Ti F) Lx Ly Lz\")\n",
    "\n",
    "        # Forward pass through model\n",
    "        model_outputs = self.model(model_inputs)\n",
    "        return model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcbf06a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cfg.train_params.ckpt_path = \"./checkpoints/well_model_constellar_data\"\n",
    "constellaration_model_trainer = ConstellarationTrainer(cfg)\n",
    "constellaration_model_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0991e3cc-0f08-4b2d-9597-3ca9baae322d",
   "metadata": {},
   "source": [
    "At this point, three distinct SciML projects have been combined into one unified interface built on top of the PhysicsNeMo framework. The combination of projects has enabled deep research and modeling opportunities that leveraging surrogate models for Stellarator design, and provides the working models for utilizing pre-trained models for a variety of down stream tasks. Each of the outlined recipes for loading datasets, models, and pre-trained models are transferable across models and datasets from The Well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f1bea-cc2c-4cc5-af23-e605dbcadb75",
   "metadata": {},
   "source": [
    "## Expanding The Scope - More Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc7168f",
   "metadata": {},
   "source": [
    "Another great feature of PhysicsNeMo is its catalog of state-of-the-art models prebuilt into the framework. This catalog expands on the available models from The Well, and provide reference implementations for a variety of physics inspired problems. The same steps outlined in this tutorial can be used to apply these models to the same dataset here, or to try out other combinations of datasets and models that have not yet been explored in The Well of PhysicsNeMo. Surrogate modeling is _not_ an easy task, and having a unified framework to enable rapid prototyping and research will lead to quicker exploration and adoption of these techniques to accelerate scientific workloads. To this end, it is encouraged utilize this framework to explore the models both in The Well and PhysicsNeMo to train a variety of surrogate models for downstream SciML tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b62434-f646-41d5-a70c-5d0f1f99c827",
   "metadata": {},
   "source": [
    "## Conclusion: Using PhysicsNeMo as a Catalyst for Collaborative SciML Research\n",
    "\n",
    "PhysicsNeMo is fundamentally transforming scientific machine learning research by serving as a powerful integration hub that bridges previously siloed community efforts. Through its interoperability with major SciML initiatives like The Well and the ConStellaration Challenge, the framework enables researchers to leverage collective knowledge for rapid research and development cycles. In this setting, PhysicsNeMo unifies diverse community resources under a single, cohesive framework, demonstrated through dataset and model integrations. The stellarator design example illustrates PhysicsNeMo's ability to orchestrate complex, multi-stage scientific workflows by combining The Well's pre-trained MHD models with the ConStellaration dataset to create surrogate models that map stellarator geometry directly to performance metrics - a task that would be prohibitively expensive through traditional numerical simulation alone. This collaborative approach enables individual research efforts become more valuable through integration, new researchers can build upon existing work more easily, and the collective knowledge of the SciML community becomes more accessible and actionable. By providing a unified interface that respects and integrates existing community efforts, PhysicsNeMo is helping to realize the full potential of collaborative scientific research in the AI/ML era, where scientific discovery is accelerated not just by individual breakthroughs, but by the seamless integration and amplification of community knowledge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
